{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c83c3f0-6a5c-43ed-b60e-47e3291e0feb",
   "metadata": {},
   "source": [
    "# Implementing Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc998a-08e0-42ef-b02d-79f3c9e9428b",
   "metadata": {},
   "source": [
    "We would cover the following:\n",
    "* Why do we need attention mechanisms in neural networK\n",
    "* Implemetin a basic self-attention framework, progressing to an enhanced self-attention mechanism\n",
    "* A causal attention module that allows LLMs to generate on token at a time\n",
    "* Masking randomly selected attention weights with dropouts to reduce overfitting\n",
    "* Stacking multiple causal attention modules into a multi-head attention module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4050b90a-9509-43d4-8b71-8685022ef135",
   "metadata": {},
   "source": [
    "### Type os Attention mechanis\n",
    "* Simplifed Self-attention\n",
    "* Self-attention\n",
    "* Causal attention\n",
    "* Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f274f2ba-b1d0-4105-b342-8a51c87feea2",
   "metadata": {},
   "source": [
    "## 3.2 Capuring Data Dependencies wity Attention Mechanism\n",
    "Initialy the Bahdanau mechanism was used with RNNs to give the current layer access to previous layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8063e4e6-c7fa-459f-9149-1a1a1a30dca4",
   "metadata": {},
   "source": [
    "**Self-attention** - This mechanism allows each position in the input sequence to consider the relevancy of (or attend\n",
    "to) all other positions in the same sequence when computing the representation of the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009786a6-44b2-498d-b772-67949160e6c5",
   "metadata": {},
   "source": [
    "## 3.3 Attending to Various Parts of the Input\n",
    "The self refers to the ability of the llm refer to weights of other tokens in the same sequence.\n",
    "The goal of self-attention is to compute the **context vector** for each each input element that combines information\n",
    "from all other inputs \\\n",
    "Take note of the following terms:\n",
    "* input elements x\n",
    "* attention scores - this is vector of how the element of each input attents to all other elements in the input sequence\n",
    "* attention weights - this is normalized attention weights\n",
    "* context vector - this a vector derive by combining each input vector with it's attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6dbf863-0841-4237-80fe-49e1724fcd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context vector calculation for the input \"Your journey starts with one step\"\n",
    "import torch\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89], # Your    (x^1)\n",
    "        [0.55, 0.87, 0.66], # journey (x^2)\n",
    "        [0.57, 0.85, 0.64], # starts  \n",
    "        [0.22, 0.58, 0.33], # with\n",
    "        [0.77, 0.25, 0.01], # one\n",
    "        [0.05, 0.80, 0.55]  #step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7513d-537c-4372-b260-974438c2a6eb",
   "metadata": {},
   "source": [
    "### How to calculate attention score \n",
    "We will calculate attention score by of the second element with reference to every other elements and this is calculated\n",
    "taking the dot product of the current element (query token) and the every other element.\n",
    "Then we would have a set of scores that would make up the context vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9e43c84-2fe5-417f-a150-a3862b69effd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9995, 1.4950, 1.4570, 0.4937, 0.6555, 0.9450])\n"
     ]
    }
   ],
   "source": [
    "# Calculating the attention scores\n",
    "attention_scores_2 = torch.empty(size=(inputs.shape[0],)) # We create an empty tensor with dimention same as that of the inputs\n",
    "for index, query in enumerate(inputs): # We iterate throught he inputs (each embedding vector in the input)\n",
    "    attention_scores_2[index] = torch.dot(query, inputs[index]) # for each input, we find the dot product of the query and the current input and store it at the index position of the attention_scores\n",
    "print(attention_scores_2) # print the attention scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1057b6-1d8d-4116-ab67-509ac29c4f12",
   "metadata": {},
   "source": [
    "**Dot Product** \\\n",
    "A dot product is anothe way to multiply two vectors element-wise and then summing the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "430bd5aa-4a6d-4569-a260-686790569ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9995)\n",
      "tensor(0.9995)\n"
     ]
    }
   ],
   "source": [
    "attention_scores2 = torch.empty(inputs.shape[0])\n",
    "query = inputs[0]\n",
    "result = 0\n",
    "for index, item in enumerate(query):\n",
    "    result = result + query[index] * item\n",
    "print(result)\n",
    "print(torch.dot(inputs[0], query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7199e5e0-ee20-482f-ae1e-328f0927680f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.3775, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.6476, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.6578, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3177, 0.6565],\n",
      "        [0.3775, 0.6476, 0.6578, 0.3177, 0.6555, 0.2440],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2440, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Calculating the attention matrix\n",
    "# This is a matrix of the attention scores of all the inputs\n",
    "attention_scores_matrix = torch.empty(inputs.shape[0], inputs.shape[0])\n",
    "for index, query in enumerate(inputs):\n",
    "    attention_scores_matrix[index] = torch.matmul(inputs, query)\n",
    "print(attention_scores_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14ab3a66-405b-4024-83f3-eb70b9557c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.3775, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.6476, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.6578, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3177, 0.6565],\n",
      "        [0.3775, 0.6476, 0.6578, 0.3177, 0.6555, 0.2440],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2440, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# We can also obtain the attention_scores_matrix by mulitplying the \n",
    "print(torch.matmul(inputs, inputs.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6653033-d699-4c88-b5de-145717df2613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.3775, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.6476, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.6578, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3177, 0.6565],\n",
      "        [0.3775, 0.6476, 0.6578, 0.3177, 0.6555, 0.2440],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2440, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs @ inputs.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31231dc-469e-494d-bef8-c9c981f5732e",
   "metadata": {},
   "source": [
    "**Dot Product and Similarity** \\\n",
    "The dot product of of two vectors is a measure of similrity because it measures how closely two vectors are aligned.\n",
    "When the dot product is high, it shows taht theres is a greate degree of similarity between the vectors.\n",
    "Therefore, in the context of self-attention, it determines the extent to which each element attends to every other element\n",
    "in the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81177094-36a3-40e8-9543-84cf80efc5fe",
   "metadata": {},
   "source": [
    "**Normalization** \\\n",
    "We would have to normilize each of the attention scores we obtained. \\\n",
    "The goal of normalization is to obtain attention weights that sum up to 1\n",
    "\\\n",
    "**Attention Weights** \\\n",
    "We obtain the attention weigths by normalizing the attention scores. This is achieved by dividing each attention score by the\n",
    "sum of all the attention scores \n",
    "\n",
    "**The Softmax Function** \\\n",
    "Using the softmax function for normalization. This function is more efficient at handling extreme values and is more\n",
    "gives more favorable gradient properties during training.\n",
    "It also ensures that teh attention weights are always positive making the output interpretable as probabilities or relative\n",
    "importance where higher weights indicates greater importance.\n",
    "Pytorch has an implementation of the softmax() fuction which is much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d562668-3307-451e-9e1a-2881a98edd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1543, 0.2533, 0.2438, 0.0931, 0.1094, 0.1461])\n",
      "Attention weights sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "## Softmax implementation\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attention_weights = softmax_naive(attention_scores)\n",
    "print(\"Attention weights: \", attention_weights)\n",
    "print(\"Attention weights sum: \", attention_weights.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b19578f0-ab89-4190-a3a1-b046a97df575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1653, 0.2473, 0.2410, 0.0817, 0.1084, 0.1563])\n"
     ]
    }
   ],
   "source": [
    "# Peform normilization\n",
    "attention_weight_2 = attention_scores_2 / sum(attention_scores_2)\n",
    "print(attention_weight_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68906340-40fa-4c3b-bfeb-630b40f03bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(attention_weight_1.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591fbfa9-1465-4169-a6c4-63f40c46f937",
   "metadata": {},
   "source": [
    "## The Context Vector\n",
    "This is like the enriched embedding vectors.\n",
    "It incorporates each input vector with the attention weights for that vector.\n",
    "To obtain this, we must multiply each input vector with the correponding attention weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f458e90-9966-46ab-829e-074aad8160d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0078, 0.1250, 0.0860])\n"
     ]
    }
   ],
   "source": [
    "# Computing the context vector\n",
    "query = inputs[1]\n",
    "context_vector_2 = torch.zeros(query.shape)\n",
    "for index, query in enumerate(inputs):\n",
    "    context_vector_2 = attention_weight_2[index] * query\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83fd7898-1156-4804-a857-478af154d3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2118, 0.2025, 0.2000, 0.1254, 0.1137, 0.1465],\n",
      "        [0.1394, 0.2394, 0.2347, 0.1248, 0.1026, 0.1591],\n",
      "        [0.1399, 0.2384, 0.2341, 0.1250, 0.1053, 0.1574],\n",
      "        [0.1441, 0.2082, 0.2053, 0.1467, 0.1231, 0.1727],\n",
      "        [0.1477, 0.1935, 0.1955, 0.1391, 0.1950, 0.1292],\n",
      "        [0.1391, 0.2194, 0.2138, 0.1427, 0.0945, 0.1905]])\n"
     ]
    }
   ],
   "source": [
    "# Normalize the attention scores matrix\n",
    "attention_weights_matrix = torch.softmax(attention_scores_matrix, dim=1)\n",
    "print(attention_weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "277a85d6-f1f9-416c-ad7a-f99b0b512b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# check that the rows sum up to 1\n",
    "for item in attention_weights_matrix:\n",
    "    print(item.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "269c7a17-edeb-4cdf-9c87-57c487ef2a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4389, 0.5964, 0.5733],\n",
      "        [0.4398, 0.6540, 0.5620],\n",
      "        [0.4411, 0.6521, 0.5605],\n",
      "        [0.4291, 0.6312, 0.5416],\n",
      "        [0.4686, 0.5895, 0.5032],\n",
      "        [0.4160, 0.6522, 0.5583]])\n"
     ]
    }
   ],
   "source": [
    "# Compute the context vector matrix\n",
    "context_vector_matrix = torch.matmul(attention_weights_matrix, inputs) # same as attention_weights_matrix @ inputs\n",
    "print(context_vector_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02ef78-2785-45e3-8187-0b9085771d61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
