{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e23b313d",
   "metadata": {},
   "source": [
    "# 3. Implementing Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc5ed20",
   "metadata": {},
   "source": [
    "We would cover the following:\n",
    "* Why do we need attention mechanisms in neural networK\n",
    "* Implemetin a basic self-attention framework, progressing to an enhanced self-attention mechanism\n",
    "* A causal attention module that allows LLMs to generate on token at a time\n",
    "* Masking randomly selected attention weights with dropouts to reduce overfitting\n",
    "* Stacking multiple causal attention modules into a multi-head attention module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f73561",
   "metadata": {},
   "source": [
    "### Type oF Attention mechanis\n",
    "* Simplifed Self-attention\n",
    "* Self-attention\n",
    "* Causal attention\n",
    "* Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deee5bb",
   "metadata": {},
   "source": [
    "## 3.2 Capuring Data Dependencies wity Attention Mechanism\n",
    "Initialy the Bahdanau mechanism was used with RNNs to give the current layer access to previous layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd799081",
   "metadata": {},
   "source": [
    "**Self-attention** - This mechanism allows each position in the input sequence to consider the relevancy of (or attend\n",
    "to) all other positions in the same sequence when computing the representation of the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314ba30",
   "metadata": {},
   "source": [
    "## 3.3 Attending to Various Parts of the Input\n",
    "The self refers to the ability of the llm refer to weights of other tokens in the same sequence.\n",
    "The goal of self-attention is to compute the **context vector** for each each input element that combines information\n",
    "from all other inputs \\\n",
    "Take note of the following terms:\n",
    "* input elements x\n",
    "* attention scores - this is vector of how the element of each input attents to all other elements in the input sequence\n",
    "* attention weights - this is normalized attention weights\n",
    "* context vector - this a vector derive by combining each input vector with it's attention weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17033b61",
   "metadata": {},
   "source": [
    "### How to calculate attention score \n",
    "We will calculate attention score by of the second element with reference to every other elements and this is calculated\n",
    "taking the dot product of the current element (query token) and the every other element.\n",
    "Then we would have a set of scores that would make up the context vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85117f4",
   "metadata": {},
   "source": [
    "# Calculating the attention scores\n",
    "attention_scores_2 = torch.empty(size=(inputs.shape[0],)) # We create an empty tensor with dimention same as that of the inputs\n",
    "for index, query in enumerate(inputs): # We iterate throught he inputs (each embedding vector in the input)\n",
    "    attention_scores_2[index] = torch.dot(query, inputs[index]) # for each input, we find the dot product of the query and the current input and store it at the index position of the attention_scores\n",
    "print(attention_scores_2) # print the attention scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553c139c",
   "metadata": {},
   "source": [
    "**Dot Product** \\\n",
    "A dot product is anothe way to multiply two vectors element-wise and then summing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70638c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores2 = torch.empty(inputs.shape[0])\n",
    "query = inputs[0]\n",
    "result = 0\n",
    "for index, item in enumerate(query):\n",
    "    result = result + query[index] * item\n",
    "print(result)\n",
    "print(torch.dot(inputs[0], query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2480c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the attention matrix\n",
    "# This is a matrix of the attention scores of all the inputs\n",
    "attention_scores_matrix = torch.empty(inputs.shape[0], inputs.shape[0])\n",
    "for index, query in enumerate(inputs):\n",
    "    attention_scores_matrix[index] = torch.matmul(inputs, query)\n",
    "print(attention_scores_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de10c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also obtain the attention_scores_matrix by mulitplying the \n",
    "print(torch.matmul(inputs, inputs.T))\n",
    "print(inputs @ inputs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f31e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context vector calculation for the input \"Your journey starts with one step\"\n",
    "import torch\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89], # Your    (x^1)\n",
    "        [0.55, 0.87, 0.66], # journey (x^2)\n",
    "        [0.57, 0.85, 0.64], # starts  \n",
    "        [0.22, 0.58, 0.33], # with\n",
    "        [0.77, 0.25, 0.01], # one\n",
    "        [0.05, 0.80, 0.55]  #step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3403162a",
   "metadata": {},
   "source": [
    "**Dot Product and Similarity** \\\n",
    "The dot product of of two vectors is a measure of similrity because it measures how closely two vectors are aligned.\n",
    "When the dot product is high, it shows taht theres is a greate degree of similarity between the vectors.\n",
    "Therefore, in the context of self-attention, it determines the extent to which each element attends to every other element\n",
    "in the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabe57f8",
   "metadata": {},
   "source": [
    "**Normalization** \\\n",
    "We would have to normilize each of the attention scores we obtained. \\\n",
    "The goal of normalization is to obtain attention weights that sum up to 1\n",
    "\\\n",
    "**Attention Weights** \\\n",
    "We obtain the attention weigths by normalizing the attention scores. This is achieved by dividing each attention score by the\n",
    "sum of all the attention scores \n",
    "\n",
    "**The Softmax Function** \\\n",
    "Using the softmax function for normalization. This function is more efficient at handling extreme values and is more\n",
    "gives more favorable gradient properties during training.\n",
    "It also ensures that teh attention weights are always positive making the output interpretable as probabilities or relative\n",
    "importance where higher weights indicates greater importance.\n",
    "Pytorch has an implementation of the softmax() fuction which is much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c9c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Softmax implementation\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attention_weights = softmax_naive(attention_scores)\n",
    "print(\"Attention weights: \", attention_weights)\n",
    "print(\"Attention weights sum: \", attention_weights.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22114e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peform normilization\n",
    "attention_weight_2 = attention_scores_2 / sum(attention_scores_2)\n",
    "print(attention_weight_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf96d5e",
   "metadata": {},
   "source": [
    "## The Context Vector\n",
    "This is like the enriched embedding vectors.\n",
    "It incorporates each input vector with the attention weights for that vector.\n",
    "To obtain this, we must multiply each input vector with the correponding attention weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c40a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the context vector\n",
    "query = inputs[1]\n",
    "context_vector_2 = torch.zeros(query.shape)\n",
    "for index, query in enumerate(inputs):\n",
    "    context_vector_2 = attention_weight_2[index] * query\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa02e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the attention scores matrix\n",
    "attention_weights_matrix = torch.softmax(attention_scores_matrix, dim=1)\n",
    "print(attention_weights_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09aed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the rows sum up to 1\n",
    "for item in attention_weights_matrix:\n",
    "    print(item.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the context vector matrix\n",
    "context_vector_matrix = torch.matmul(attention_weights_matrix, inputs) # same as attention_weights_matrix @ inputs\n",
    "print(context_vector_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5667ec55-3a2e-4730-a4a9-415d6971ed08",
   "metadata": {},
   "source": [
    "## 3.4 Self-Attention With Trainable Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ef1992-ec28-4b27-b7cd-eacd00e0137e",
   "metadata": {},
   "source": [
    "To implement the self-attention mechanisme, we would need to introduce the trainable weight matrices\n",
    "Wq, Wk and Wv\n",
    "These three matrices are used to project the embedded input tokens into query, key and value vectors respectively /\n",
    "In the first step of the self-attention mechanism with traninable weights, we compute:\n",
    "* query (q)\n",
    "* key (k)\n",
    "* value (v) \\\n",
    "for the input elements x \\\n",
    "\n",
    "We designate the second input x2 as the query input\n",
    "* the query vector is obtained by matrix multiplication between the input and the weight matrix Wq\n",
    "* the key vector is obtained by matrix multiplication between the input and the weight matrix Wk\n",
    "* the value vector is obtained by a matriv multiplication between the input and weight matrix Wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b98283a-baa0-4c01-9532-b243ff4ddbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#initialize the inputs\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89], # Your    (x^1)\n",
    "        [0.55, 0.87, 0.66], # journey (x^2)\n",
    "        [0.57, 0.85, 0.64], # starts  \n",
    "        [0.22, 0.58, 0.33], # with\n",
    "        [0.77, 0.25, 0.01], # one\n",
    "        [0.05, 0.80, 0.55]  #step\n",
    "    ]\n",
    ")\n",
    "\n",
    "x_2 = inputs[1] #the second element of the inputs\n",
    "d_in = inputs.shape[1] # the size of the input embedding, d=3\n",
    "d_out = 2 # the output embedding size, d_out=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98471852-0aad-4c58-a570-a88c0e1dcf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n"
     ]
    }
   ],
   "source": [
    "print(x_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "454e2d1b-fb27-4b94-b82e-be7713bb9496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weight matrices\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "# Note that we set the requires_grad parameter to False, but if we would use the weight matrices for training, we set to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3495f4a6-7a6f-4781-90ac-3ece1c25a4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.1366, 0.1025],\n",
       "        [0.1841, 0.7264],\n",
       "        [0.3153, 0.6871]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e371dcc-fdb5-4ab0-83f3-d7735d8fda51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now comput the query, key and value vectors\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7681660-eabd-457c-ad76-7a99e8287c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n",
      "tensor([0.4433, 1.1419])\n",
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "print(query_2)\n",
    "print(key_2)\n",
    "score_2 = query_2.dot(key_2)\n",
    "print(score_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9aa47d-8977-4d78-8265-c915dc894d2e",
   "metadata": {},
   "source": [
    "## Weight Parameters vs Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36313576-e960-47e4-a47b-9fc1717fa621",
   "metadata": {},
   "source": [
    "The weight matrices here is not the same as the attention weights.\n",
    "The weight parameters of a network are the values that are optimized during the training phase\n",
    "Attention weights are the values that determine the extent to which the context vector depends on other parts of the input \\\n",
    "Weight parameters are teh fundamental learnt coefficients that define the network structur while attention weights are\n",
    "dynamic context-specific values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be984c1c-c37f-42db-8719-9baabca5117d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Obtain all keys and values\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecea9ef1-40b5-4a52-9cc5-9a1bc13e2133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3669, 0.7646],\n",
       "        [0.4433, 1.1419],\n",
       "        [0.4361, 1.1156],\n",
       "        [0.2408, 0.6706],\n",
       "        [0.1543, 0.2674],\n",
       "        [0.3275, 0.9642]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff19dac-9640-4dbc-80fd-3657cb5ec3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4433, 1.1419])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "keys_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8524254-53e8-465a-9e06-b314f0c3392f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "# Compute the attention scores for input 2 against keys_2\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f3abfa9-6a10-46f3-b03d-021c5dfb17c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.4555, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "# Compute the attention scores for the given query\n",
    "attention_scores_2 = query_2 @ keys.T\n",
    "print(attention_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf066f1e-293d-4025-b608-14f7eaafd2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1510, 0.2278, 0.2213, 0.1319, 0.0848, 0.1832])\n"
     ]
    }
   ],
   "source": [
    "# We now need to calculate the attention weights\n",
    "# this is done by scaling the attention scores by dividing them by the square root of the embedding dimention of the keys\n",
    "\n",
    "d_k = keys.shape[-1] # calculate the size of the last dimension of the keys tensor\n",
    "attn_weight_2 = torch.softmax(attention_scores_2/ d_k**0.5, dim=-1) # dk**0.5 is the scaling factor and used to stabilize the softmax computation\n",
    "print(attn_weight_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82db28e0-704c-4eae-9cfb-599afe15fb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3062, 0.8178])\n"
     ]
    }
   ],
   "source": [
    "# Compute the context vector by combining all value vectors via the attention weights\n",
    "context_vec_2 = attn_weight_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe26639",
   "metadata": {},
   "source": [
    "## Assignment 1: Compute all context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2cd3552-3c3f-4311-96b7-c2a898d1cc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2993, 0.8003],\n",
       "        [0.3062, 0.8178],\n",
       "        [0.3059, 0.8170],\n",
       "        [0.2942, 0.7874],\n",
       "        [0.2906, 0.7782],\n",
       "        [0.2987, 0.7989]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution to Assignment 1\n",
    "context_vectors = torch.empty(inputs.size(0), W_value.size(1))\n",
    "for index, item in enumerate(inputs):\n",
    "    # compute the query\n",
    "    query = item @ W_query\n",
    "    key = inputs @ W_key\n",
    "    value = inputs @ W_value\n",
    "\n",
    "    # compute the attention scores across the keys\n",
    "    attention_scores = query @ key.T\n",
    "\n",
    "    # calculate the attention weight\n",
    "    key_dim = key.shape[-1]\n",
    "    attention_weights = torch.softmax(attention_scores/key_dim**0.5, dim=-1)\n",
    "\n",
    "    # compute the context vector\n",
    "    context_vector = attention_weights @ value\n",
    "\n",
    "    # add the computed context vector to postion index of the context_vectors\n",
    "    context_vectors[index] = context_vector\n",
    "\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5591116-f490-4b2f-a639-9b09ae8fa7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2993, 0.8003],\n",
      "        [0.3062, 0.8178],\n",
      "        [0.3059, 0.8170],\n",
      "        [0.2942, 0.7874],\n",
      "        [0.2906, 0.7782],\n",
      "        [0.2987, 0.7989]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Using our concise self-attention class v1\n",
    "import torch\n",
    "from utilities.SelfAttention import SelfAttention_v1\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1636cc5c-7872-410d-95b4-591f74c3e902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0715,  0.0736],\n",
      "        [-0.0729,  0.0720],\n",
      "        [-0.0730,  0.0719],\n",
      "        [-0.0737,  0.0707],\n",
      "        [-0.0742,  0.0699],\n",
      "        [-0.0733,  0.0713]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Using the improved self-attention class v2\n",
    "from utilities.SelfAttention import SelfAttention_v2\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8ac787-097b-4372-a798-f2be09382e16",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "Transfer the weight matrices from an instance of SelfAttention_v1 to and instance of SelfAttention_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f949d",
   "metadata": {},
   "source": [
    "### Solution to Assignment 2\n",
    "To transfer the weight matrices from an instance of SelfAttention_v2 to an instance of SelfAttention_v1, you will have the map the weights of the nn.Linear layers in SelfAttention_v2\n",
    "to the nn.Parameter tensors in SelfAttention_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d42de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sa_v1.W_query.data = sa_v2.W_query.weight.clone().T\n",
    "    sa_v1.W_key.data = sa_v2.W_keys.weight.clone().T\n",
    "    sa_v1.W_value.data = sa_v2.W_values.weight.clone().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fb78aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0715,  0.0736],\n",
      "        [-0.0729,  0.0720],\n",
      "        [-0.0730,  0.0719],\n",
      "        [-0.0737,  0.0707],\n",
      "        [-0.0742,  0.0699],\n",
      "        [-0.0733,  0.0713]], grad_fn=<MmBackward0>)\n",
      "tensor([[-0.0715,  0.0736],\n",
      "        [-0.0729,  0.0720],\n",
      "        [-0.0730,  0.0719],\n",
      "        [-0.0737,  0.0707],\n",
      "        [-0.0742,  0.0699],\n",
      "        [-0.0733,  0.0713]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output_v1 = sa_v1(inputs)\n",
    "output_v2 = sa_v2(inputs)\n",
    "print(output_v1)\n",
    "print(output_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0267ff-86ff-4b89-b39a-7db0da6ef72b",
   "metadata": {},
   "source": [
    "## 3.5 Using Causal Attention (Masked Attention)\n",
    "This used to ensure that only the tokens that appear prior to the current position are considered\n",
    "when predicting the next token in the sequence.\n",
    "This is called causal attention and is a specialized from of self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707e2a6-0a63-4cd6-943b-d395744e3234",
   "metadata": {},
   "source": [
    "For each token in the input, we mask out future tokens and restricts access to only the previous and \n",
    "current inputs in the sequence. In contrast, the standard self-attention mechanism allows access to the entire\n",
    "input sequence at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0753157f-ec4c-4303-b023-d00ffc758b1b",
   "metadata": {},
   "source": [
    "* Step 1 - we mask out the attention weights above the diagonal\n",
    "* Step 2 - we normalize existing attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4240bd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query from v1\n",
      "tensor([[ 0.6600, -0.2047],\n",
      "        [ 0.9091, -0.4471],\n",
      "        [ 0.8960, -0.4419],\n",
      "        [ 0.5034, -0.2633],\n",
      "        [ 0.3627, -0.2150],\n",
      "        [ 0.6628, -0.3292]], grad_fn=<MmBackward0>)\n",
      "Query from v2\n",
      "tensor([[ 0.6600, -0.2047],\n",
      "        [ 0.9091, -0.4471],\n",
      "        [ 0.8960, -0.4419],\n",
      "        [ 0.5034, -0.2633],\n",
      "        [ 0.3627, -0.2150],\n",
      "        [ 0.6628, -0.3292]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Query from v1\")\n",
    "print(inputs @ sa_v1.W_query)\n",
    "\n",
    "print(\"Query from v2\") \n",
    "print(sa_v2.W_query(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2b82f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys from v1\n",
      "tensor([[0.2309, 1.0966],\n",
      "        [0.4306, 1.4551],\n",
      "        [0.4300, 1.4343],\n",
      "        [0.2355, 0.7990],\n",
      "        [0.2917, 0.5786],\n",
      "        [0.2568, 1.0533]], grad_fn=<MmBackward0>)\n",
      "Keys from v2\n",
      "tensor([[ 0.3147, -0.4016],\n",
      "        [-0.0298, -0.4459],\n",
      "        [-0.0170, -0.4262],\n",
      "        [-0.1054, -0.2724],\n",
      "        [ 0.1972,  0.0942],\n",
      "        [-0.2258, -0.4782]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Keys from v1\")\n",
    "print(inputs @ sa_v1.W_query)\n",
    "\n",
    "print(\"Keys from v2\")\n",
    "print(sa_v2.W_keys(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76199861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values from v1\n",
      "tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1543, 0.2674],\n",
      "        [0.3275, 0.9642]], grad_fn=<MmBackward0>)\n",
      "Values from v2\n",
      "tensor([[-0.0872,  0.0286],\n",
      "        [-0.1137,  0.0766],\n",
      "        [-0.1018,  0.0927],\n",
      "        [-0.0912, -0.0026],\n",
      "        [ 0.1571,  0.3764],\n",
      "        [-0.2085, -0.1546]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Values from v1\")\n",
    "print(inputs @ sa_v1.W_key)\n",
    "\n",
    "print(\"Values from v2\")\n",
    "print(sa_v2.W_values(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83a789d6-a652-4cd2-8167-62fb14e7c966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1927, 0.1651, 0.1656, 0.1554, 0.1698, 0.1514],\n",
      "        [0.2051, 0.1667, 0.1670, 0.1503, 0.1626, 0.1484],\n",
      "        [0.2045, 0.1667, 0.1670, 0.1505, 0.1626, 0.1487],\n",
      "        [0.1874, 0.1671, 0.1673, 0.1575, 0.1639, 0.1568],\n",
      "        [0.1817, 0.1674, 0.1675, 0.1599, 0.1635, 0.1600],\n",
      "        [0.1941, 0.1669, 0.1671, 0.1547, 0.1637, 0.1534]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_keys(inputs)\n",
    "attention_scores = queries @ keys.T\n",
    "attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c289162",
   "metadata": {},
   "source": [
    "## Obtain Masked Attention Weights\n",
    "One way to obtain the masked attention weight matrix in a causal attention is to apply the softmax function to the attention scores,\n",
    "zeroing out the elements above the diagonal and normalizing the resulting matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f8c279d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1927, 0.1651, 0.1656, 0.1554, 0.1698, 0.1514],\n",
      "        [0.2051, 0.1667, 0.1670, 0.1503, 0.1626, 0.1484],\n",
      "        [0.2045, 0.1667, 0.1670, 0.1505, 0.1626, 0.1487],\n",
      "        [0.1874, 0.1671, 0.1673, 0.1575, 0.1639, 0.1568],\n",
      "        [0.1817, 0.1674, 0.1675, 0.1599, 0.1635, 0.1600],\n",
      "        [0.1941, 0.1669, 0.1671, 0.1547, 0.1637, 0.1534]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Step 1 - We first obtain the weights\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_keys(inputs)\n",
    "values = sa_v2.W_values(inputs)\n",
    "\n",
    "attention_scores = queries @ keys.T\n",
    "attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abe5bf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#Step 2 - Create a mask using the tril function such that the values above the diagonal are zero\n",
    "context_length = attention_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "476c676c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1927, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2051, 0.1667, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2045, 0.1667, 0.1670, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1874, 0.1671, 0.1673, 0.1575, 0.0000, 0.0000],\n",
      "        [0.1817, 0.1674, 0.1675, 0.1599, 0.1635, 0.0000],\n",
      "        [0.1941, 0.1669, 0.1671, 0.1547, 0.1637, 0.1534]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Step 3 - Multiply the attention weights with this mask to zero out the diagonals\n",
    "masked_attention_weights = attention_weights * mask_simple\n",
    "print(masked_attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d2ec2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2163, 0.1993, 0.1994, 0.1904, 0.1946, 0.0000],\n",
      "        [0.1941, 0.1669, 0.1671, 0.1547, 0.1637, 0.1534]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Step 4 - Renormalize the masked attention weights\n",
    "row_sums = masked_attention_weights.sum(dim=-1, keepdim=True)\n",
    "masked_attention_weights_normalized = masked_attention_weights / row_sums\n",
    "print(masked_attention_weights_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72237560",
   "metadata": {},
   "source": [
    "### More Efficient Masked Attention Weights\n",
    "A more efficient way to obtain masked attention weight matrix in causal attention is to mask the attention scores with negative values before applying the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90ddb70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2005, 0.0850, 0.0855, 0.0203, 0.0513,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.0997, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ccd26fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2163, 0.1993, 0.1994, 0.1904, 0.1946, 0.0000],\n",
      "        [0.1941, 0.1669, 0.1671, 0.1547, 0.1637, 0.1534]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9027bf",
   "metadata": {},
   "source": [
    "## Masking Additional Weights With Dropout\n",
    "Could be applied after computing the attention weights\n",
    "This is done to reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5ad3ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Example of dropout\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) # we choose a dropout rate of 50%\n",
    "example = torch.ones(6,6)\n",
    "print(dropout(example))\n",
    "\n",
    "# Note that in the output, the tensor contains twos and this is because then non-dropped-out elements are scaled by a factor of 1/(1-p) where p is the scaling factor\n",
    "# In this case the scaling factor is 0.5. So after dropout, the remaining ones are scaled by 1/(1-0.5) = 2\n",
    "# Therefore, we have twos instead of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1db133f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4325, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3338, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's now apply dropout to the attention weights matrix\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "attention_weights_do = dropout(attention_weights)\n",
    "print(attention_weights_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08f4e447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Duplicating the example to create 2 batches\n",
    "# There would be 2 inputs with 6 tokens each where each token has an embedding dimension of 3\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8bca0140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#initialize the inputs\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89], # Your    (x^1)\n",
    "        [0.55, 0.87, 0.66], # journey (x^2)\n",
    "        [0.57, 0.85, 0.64], # starts  \n",
    "        [0.22, 0.58, 0.33], # with\n",
    "        [0.77, 0.25, 0.01], # one\n",
    "        [0.05, 0.80, 0.55]  #step\n",
    "    ]\n",
    ")\n",
    "\n",
    "x_2 = inputs[1] #the second element of the inputs\n",
    "d_in = inputs.shape[1] # the size of the input embedding, d=3\n",
    "d_out = 2 # the output embedding size, d_out=2\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f3eeca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Utilizing the CausalAttention class\n",
    "\n",
    "from utilities.SelfAttention import CausalAttention\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vectors = ca(batch)\n",
    "print(context_vector.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de4ec7",
   "metadata": {},
   "source": [
    "## 3.6 Muli-head Attention\n",
    "This maeas that we divide the attention mechanism into multipoe heads each operating independently.\n",
    "Multi-head attention could consist of several single-head attention modules stacked on top of each other.\n",
    "For example, let's say we have two single head attention module. In this case, instead of using a singel matrix Wv for computing the value matrices in a multi-head attention module with two heads, we now have two value weight matrices Wv1 and Wv2. Same applies to Wq and Wk.\n",
    "This means that we also obtain two sets of vectors Z1 and Z2 that we can combine into a single vector matrix Z."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
